{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate parquet datasets from original datasets\n",
    "\n",
    "This notebook generates parquet datasets from original datasets, which ar stored in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "ProgressBar().register()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA_BLOCKSIZE = 2**27 # 128 MB\n",
    "\n",
    "PATH_DATA = '../data/'\n",
    "\n",
    "PATH_METADATA_SRC  = PATH_DATA + 'yt_metadata_en.jsonl'\n",
    "PATH_METADATA_DST  = PATH_DATA + 'yt_metadata_en.parquet/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {\n",
    "    'categories'    : 'str',\n",
    "    'channel_id'    : 'str',\n",
    "    'crawl_date'    : 'str',\n",
    "    'description'   : 'str',\n",
    "    'dislike_count' : 'int32',\n",
    "    'display_id'    : 'str',\n",
    "    'duration'      : 'int32',\n",
    "    'like_count'    : 'int32',\n",
    "    'tags'          : 'str',\n",
    "    'title'         : 'str',\n",
    "    'upload_date'   : 'str',\n",
    "    'view_count'    : 'int32'\n",
    "}\n",
    "df = dd.read_json(PATH_METADATA_SRC, lines=True, blocksize=METADATA_BLOCKSIZE, dtype=dtype, convert_dates=['crawl_date', 'upload_date'])\n",
    "df['categories'] = df['categories'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[#######                                 ] | 18% Completed | 11min 27.6s\n",
      "[#######                                 ] | 18% Completed | 11min 27.7s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The columns in the computed data do not match the columns in the provided metadata\nOrder of columns does not match",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Cours\\EPFL\\IN Master 1\\Applied Data Analysis [CS401]\\Project\\processing\\generate_datasets.ipynb Cellule 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Cours/EPFL/IN%20Master%201/Applied%20Data%20Analysis%20%5BCS401%5D/Project/processing/generate_datasets.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df\u001b[39m.\u001b[39;49mto_parquet(PATH_METADATA_DST, engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpyarrow\u001b[39;49m\u001b[39m'\u001b[39;49m, compression\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msnappy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\ada\\lib\\site-packages\\dask\\dataframe\\core.py:4825\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[1;34m(self, path, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4822\u001b[0m \u001b[39m\"\"\"See dd.to_parquet docstring for more information\"\"\"\u001b[39;00m\n\u001b[0;32m   4823\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[1;32m-> 4825\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\u001b[39mself\u001b[39m, path, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\ada\\lib\\site-packages\\dask\\dataframe\\io\\parquet\\core.py:840\u001b[0m, in \u001b[0;36mto_parquet\u001b[1;34m(df, path, engine, compression, write_index, append, overwrite, ignore_divisions, partition_on, storage_options, custom_metadata, write_metadata_file, compute, compute_kwargs, schema, name_function, **kwargs)\u001b[0m\n\u001b[0;32m    838\u001b[0m graph \u001b[39m=\u001b[39m HighLevelGraph\u001b[39m.\u001b[39mfrom_collections(meta_name, dsk, dependencies\u001b[39m=\u001b[39m(data_write,))\n\u001b[0;32m    839\u001b[0m \u001b[39mif\u001b[39;00m compute:\n\u001b[1;32m--> 840\u001b[0m     \u001b[39mreturn\u001b[39;00m compute_as_if_collection(\n\u001b[0;32m    841\u001b[0m         Scalar, graph, [(meta_name, \u001b[39m0\u001b[39m)], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcompute_kwargs\n\u001b[0;32m    842\u001b[0m     )\n\u001b[0;32m    843\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    844\u001b[0m     \u001b[39mreturn\u001b[39;00m Scalar(graph, meta_name, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\ada\\lib\\site-packages\\dask\\base.py:317\u001b[0m, in \u001b[0;36mcompute_as_if_collection\u001b[1;34m(cls, dsk, keys, scheduler, get, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m schedule \u001b[39m=\u001b[39m get_scheduler(scheduler\u001b[39m=\u001b[39mscheduler, \u001b[39mcls\u001b[39m\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m, get\u001b[39m=\u001b[39mget)\n\u001b[0;32m    316\u001b[0m dsk2 \u001b[39m=\u001b[39m optimization_function(\u001b[39mcls\u001b[39m)(dsk, keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 317\u001b[0m \u001b[39mreturn\u001b[39;00m schedule(dsk2, keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\ada\\lib\\site-packages\\dask\\threaded.py:81\u001b[0m, in \u001b[0;36mget\u001b[1;34m(dsk, result, cache, num_workers, pool, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(pool, multiprocessing\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mPool):\n\u001b[0;32m     79\u001b[0m         pool \u001b[39m=\u001b[39m MultiprocessingPoolExecutor(pool)\n\u001b[1;32m---> 81\u001b[0m results \u001b[39m=\u001b[39m get_async(\n\u001b[0;32m     82\u001b[0m     pool\u001b[39m.\u001b[39msubmit,\n\u001b[0;32m     83\u001b[0m     pool\u001b[39m.\u001b[39m_max_workers,\n\u001b[0;32m     84\u001b[0m     dsk,\n\u001b[0;32m     85\u001b[0m     result,\n\u001b[0;32m     86\u001b[0m     cache\u001b[39m=\u001b[39mcache,\n\u001b[0;32m     87\u001b[0m     get_id\u001b[39m=\u001b[39m_thread_get_id,\n\u001b[0;32m     88\u001b[0m     pack_exception\u001b[39m=\u001b[39mpack_exception,\n\u001b[0;32m     89\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m     90\u001b[0m )\n\u001b[0;32m     92\u001b[0m \u001b[39m# Cleanup pools associated to dead threads\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[39mwith\u001b[39;00m pools_lock:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\ada\\lib\\site-packages\\dask\\local.py:506\u001b[0m, in \u001b[0;36mget_async\u001b[1;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[0;32m    504\u001b[0m         _execute_task(task, data)  \u001b[39m# Re-execute locally\u001b[39;00m\n\u001b[0;32m    505\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 506\u001b[0m         raise_exception(exc, tb)\n\u001b[0;32m    507\u001b[0m res, worker_id \u001b[39m=\u001b[39m loads(res_info)\n\u001b[0;32m    508\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mcache\u001b[39m\u001b[39m\"\u001b[39m][key] \u001b[39m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\ada\\lib\\site-packages\\dask\\local.py:314\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(exc, tb)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39mif\u001b[39;00m exc\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[0;32m    313\u001b[0m     \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[1;32m--> 314\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\ada\\lib\\site-packages\\dask\\local.py:219\u001b[0m, in \u001b[0;36mexecute_task\u001b[1;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m     task, data \u001b[39m=\u001b[39m loads(task_info)\n\u001b[1;32m--> 219\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, data)\n\u001b[0;32m    220\u001b[0m     \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m get_id()\n\u001b[0;32m    221\u001b[0m     result \u001b[39m=\u001b[39m dumps((result, \u001b[39mid\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\ada\\lib\\site-packages\\dask\\core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[1;34m(arg, cache, dsk)\u001b[0m\n\u001b[0;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[0;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[0;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m(_execute_task(a, cache) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m args))\n\u001b[0;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[0;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\ada\\lib\\site-packages\\dask\\dataframe\\utils.py:397\u001b[0m, in \u001b[0;36mcheck_meta\u001b[1;34m(x, meta, funcname, numeric_equal)\u001b[0m\n\u001b[0;32m    392\u001b[0m         errmsg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPartition type: `\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    393\u001b[0m             typename(\u001b[39mtype\u001b[39m(meta)),\n\u001b[0;32m    394\u001b[0m             asciitable([\u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mFound\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mExpected\u001b[39m\u001b[39m\"\u001b[39m], bad_dtypes),\n\u001b[0;32m    395\u001b[0m         )\n\u001b[0;32m    396\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 397\u001b[0m         check_matching_columns(meta, x)\n\u001b[0;32m    398\u001b[0m         \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m    399\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\ada\\lib\\site-packages\\dask\\dataframe\\utils.py:422\u001b[0m, in \u001b[0;36mcheck_matching_columns\u001b[1;34m(meta, actual)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    421\u001b[0m     extra_info \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOrder of columns does not match\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 422\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    423\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThe columns in the computed data do not match\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    424\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m the columns in the provided metadata\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    425\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mextra_info\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    426\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: The columns in the computed data do not match the columns in the provided metadata\nOrder of columns does not match"
     ]
    }
   ],
   "source": [
    "df.to_parquet(PATH_METADATA_DST, engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ada')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93474a10e1612df951114730cd5f83d078f687a48e5e0a7160d033353df33aec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
