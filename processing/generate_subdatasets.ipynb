{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate subdatasets from the original dataset\n",
    "\n",
    "This notebook generates subdatasets from the original dataset, to be used to test our implementations without having to wait for the whole dataset to be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = '../data/'\n",
    "PATH_SUBDATA = '../data/subdata/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATAS_SAMPLE_RATIO = 0.0001\n",
    "METADATAS_SAMPLE_SEED = 0\n",
    "\n",
    "METADATAS_FILE = 'yt_metadata_en'\n",
    "PATH_METADATAS_SRC = PATH_DATA + METADATAS_FILE + '.jsonl.gz'\n",
    "PATH_METADATAS_DST = PATH_SUBDATA + METADATAS_FILE + '_sub'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72924794"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadatas = spark.read.json(PATH_METADATAS_SRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_metadatas = metadatas.sample(False, METADATAS_SAMPLE_RATIO, seed=METADATAS_SAMPLE_SEED)\n",
    "sub_metadatas.write.json(PATH_METADATAS_DST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7042"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.json(PATH_METADATAS_DST).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS_FILE = 'df_channels_en'\n",
    "PATH_CHANNELS_SRC = PATH_DATA + CHANNELS_FILE + '.tsv.gz'\n",
    "PATH_CHANNELS_DST = PATH_SUBDATA + CHANNELS_FILE + '_sub'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136470"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channels = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('sep', '\\t') \\\n",
    "    .csv(PATH_CHANNELS_SRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_channels_metadata = sub_metadatas.select('channel_id').distinct()\n",
    "sub_channels = channels.join(sub_channels_metadata, channels.channel == sub_channels_metadata.channel_id, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_channels.write \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('sep', '\\t') \\\n",
    "    .csv(PATH_CHANNELS_DST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5699"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.json(PATH_CHANNELS_DST).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESERIES_FILE = 'df_timeseries_en'\n",
    "PATH_TIMESERIES_SRC = PATH_DATA + TIMESERIES_FILE + '.tsv.gz'\n",
    "PATH_TIMESERIES_DST = PATH_SUBDATA + TIMESERIES_FILE + '_sub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18872499"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('sep', '\\t') \\\n",
    "    .csv(PATH_TIMESERIES_SRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_time_series = time_series.join(sub_channels_metadata, time_series.channel == sub_channels_metadata.channel_id, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_time_series.write \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('sep', '\\t') \\\n",
    "    .json(PATH_TIMESERIES_DST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "868191"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.json(PATH_TIMESERIES_DST).count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('ada')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93474a10e1612df951114730cd5f83d078f687a48e5e0a7160d033353df33aec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
