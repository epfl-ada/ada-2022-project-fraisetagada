{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolve Google links in video description\n",
    "\n",
    "This notebook is part of the preprocessing pipeline for the links in video description. It resolves shortened links using GET requests to the [Google](https://goo.gl) website and saves the results in a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape Google links, we use the [Amazon AWS Gateway](https://aws.amazon.com/fr/) to change our IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting API gateways in 10 regions.\n",
      "Using 10 endpoints with name 'https://api.ipify.org - IP Rotate API' (0 new).\n",
      "Starting API gateways in 10 regions.\n",
      "Using 10 endpoints with name 'https://goo.gl - IP Rotate API' (0 new).\n",
      "Starting API gateways in 10 regions.\n",
      "Using 10 endpoints with name 'http://goo.gl - IP Rotate API' (0 new).\n"
     ]
    }
   ],
   "source": [
    "from requests_ip_rotator import ApiGateway\n",
    "\n",
    "session_urls = [\n",
    "    'https://api.ipify.org',\n",
    "    'https://goo.gl',\n",
    "    'http://goo.gl'\n",
    "]\n",
    "url_gateways = [(session_url, ApiGateway(session_url)) for session_url in session_urls]\n",
    "\n",
    "sess = requests.Session()\n",
    "for url, gateway in url_gateways:\n",
    "    gateway.start()\n",
    "    sess.mount(url, gateway)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ip(session):\n",
    "    \"\"\" Get the IP address of the current session. \"\"\"\n",
    "    return session.get('https://api.ipify.org').text\n",
    "\n",
    "# The default session is not using the proxy\n",
    "assert get_ip(requests.Session()) == get_ip(requests.Session())\n",
    "\n",
    "# The session with the proxy is changing IP addresses\n",
    "assert get_ip(sess) != get_ip(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FIX_RESOLVED_URLS = 1000    # The maximum number of URLs to fix in a single run\n",
    "REQUEST_TIMEOUT = 5             # The timeout for get requests\n",
    "\n",
    "PATH_GOOGL_URLS = '../data/generated/googl_urls.tsv'\n",
    "PATH_GOOGL_RESOLVED_URLS = '../data/generated/googl_resolved_urls.tsv'\n",
    "PATH_GOOGL_RESOLVED_URLS_TMP = '../data/generated/googl_resolved_urls_tmp.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_googl_url(url):\n",
    "    \"\"\" Resolve a goo.gl URL using GET requests.\n",
    "    \n",
    "    Args:\n",
    "        url (str): The URL to resolve.\n",
    "        \n",
    "    Returns:\n",
    "        str: The resolved URL or None if the URL could not be resolved.\n",
    "    \"\"\"\n",
    "    \n",
    "    if url is None:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        output = requests.get(url, allow_redirects=True, timeout=REQUEST_TIMEOUT)\n",
    "        return output.url\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_future_results(url_futures, file, use_tqdm=True):\n",
    "    \"\"\" Wait for the completion of the futures and write the results to the file.\n",
    "    \n",
    "    Args:\n",
    "        url_futures (list): the list of futures\n",
    "        file (str): the file to write the results to\n",
    "        use_tqdm (bool): whether to use the tqdm progress bar\n",
    "\n",
    "    Returns:\n",
    "        n_retrieved_urls (int): the number of URLs that have been retrieved\n",
    "        n_errors (int): the number of errors that have been encountered\n",
    "    \"\"\"\n",
    "    \n",
    "    n_retrieved_urls = 0\n",
    "    n_errors = 0\n",
    "    for (url, future) in tqdm(url_futures) if use_tqdm else url_futures:\n",
    "        long_url = future.result()\n",
    "        file.write(f'{url}\\t{long_url}\\n')\n",
    "        if long_url is None:\n",
    "            n_errors += 1\n",
    "        else:\n",
    "            n_retrieved_urls += 1\n",
    "    return n_retrieved_urls, n_errors\n",
    "\n",
    "def resolve_googl_urls(batch_size):\n",
    "    \"\"\" Resolves a batch of bitly URLs to their original URLs, and append the results to the resolved bitly URLs file.\n",
    "    \n",
    "    Args:\n",
    "        batch_size (int): the number of URLs to resolve in this batch\n",
    "    \"\"\"\n",
    "\n",
    "    # Count the number of already resolved URLs\n",
    "    with open(PATH_GOOGL_RESOLVED_URLS, 'r', encoding='utf-8') as f_res:\n",
    "        n_resolved_urls = sum(1 for _ in f_res)\n",
    "\n",
    "    # Count the number of URLs to resolve\n",
    "    with open(PATH_GOOGL_URLS, 'r', encoding='utf-8') as f_res:\n",
    "        n_to_resolve_urls = sum(1 for _ in f_res)\n",
    "\n",
    "    with open(PATH_GOOGL_URLS, 'r', encoding='utf-8') as f, \\\n",
    "         open(PATH_GOOGL_RESOLVED_URLS, 'a', encoding='utf-8') as f_res, \\\n",
    "         ThreadPoolExecutor() as executor:\n",
    "\n",
    "        n_retrieved_urls = 0\n",
    "        n_errors = 0\n",
    "\n",
    "        url_futures = []\n",
    "        reached_max = False\n",
    "        for idx, url in tqdm(enumerate(f.read().splitlines())):\n",
    "            # Skip the already resolved URLs\n",
    "            if idx < n_resolved_urls:\n",
    "                continue\n",
    "            # Stop if we have reached the maximum number of URLs to resolve\n",
    "            if idx >= n_resolved_urls + batch_size:\n",
    "                reached_max = True\n",
    "                break\n",
    "\n",
    "            # Get futures for the URLs\n",
    "            url_futures.append((url, executor.submit(resolve_googl_url, url)))\n",
    "        \n",
    "        # Write the results of the futures\n",
    "        n_retrieved_urls, n_errors = write_future_results(url_futures, f_res)\n",
    "        \n",
    "        if reached_max:\n",
    "            print(f'Reached max lines read: {batch_size}, {n_resolved_urls + batch_size} urls resolved out of {n_to_resolve_urls} in total.')\n",
    "        else:\n",
    "            print(f'Finished resolving all urls.')\n",
    "        print(f'Retrieved {n_retrieved_urls} urls, {n_errors} urls could not be retrieved. If the number of errors is too high, consider waiting a few moments before retrying.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_resolved_googl_urls():\n",
    "    \"\"\" Fixes the remaining bitly URLs that could not be resolved in the first step, and overwrite the resolved bitly URLs file. \"\"\"\n",
    "\n",
    "    # Delete the tmp file if it exists\n",
    "    try:\n",
    "        os.remove(PATH_GOOGL_RESOLVED_URLS_TMP)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    with open(PATH_GOOGL_RESOLVED_URLS, 'r', encoding='utf-8') as f_res, \\\n",
    "         open(PATH_GOOGL_RESOLVED_URLS_TMP, 'w', encoding='utf-8') as f_tmp, \\\n",
    "         ThreadPoolExecutor() as executor:\n",
    "                n_retrieved_urls = 0\n",
    "                n_errors = 0\n",
    "\n",
    "                url_futures = []\n",
    "                for idx, url_rurl in tqdm(enumerate(f_res.read().splitlines())):\n",
    "                    if len(url_rurl.split('\\t')) != 2:\n",
    "                        continue\n",
    "                    \n",
    "                    url, rurl = url_rurl.split('\\t')\n",
    "                    if rurl == 'None':\n",
    "                        url_futures.append((url, executor.submit(resolve_googl_url, url)))\n",
    "                    else:\n",
    "                        f_tmp.write(f'{url}\\t{rurl}\\n')\n",
    "                    \n",
    "                    # Write the results of the futures every MAX_FIX_RESOLVED_URLS iterations\n",
    "                    if idx % MAX_FIX_RESOLVED_URLS == 0 and idx != 0:\n",
    "                        n_retrieved_urls_batch, n_errors_batch = write_future_results(url_futures, f_tmp, use_tqdm=False)\n",
    "                        n_retrieved_urls += n_retrieved_urls_batch\n",
    "                        n_errors += n_errors_batch\n",
    "                        url_futures = []\n",
    "                    \n",
    "                n_retrieved_urls_batch, n_errors_batch = write_future_results(url_futures, f_tmp, use_tqdm=False)\n",
    "                n_retrieved_urls += n_retrieved_urls_batch\n",
    "                n_errors += n_errors_batch\n",
    "                \n",
    "    # Replace the old file with the new one\n",
    "    os.remove(PATH_GOOGL_RESOLVED_URLS)\n",
    "    os.rename(PATH_GOOGL_RESOLVED_URLS_TMP, PATH_GOOGL_RESOLVED_URLS)\n",
    "\n",
    "    print(f'Retrieved {n_retrieved_urls} urls, {n_errors} urls could not be retrieved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_none_urls_count():\n",
    "    \"\"\" Returns the number of URLs that could not be resolved in the resolved bitly URLs file. \"\"\"\n",
    "    \n",
    "    with open(PATH_GOOGL_RESOLVED_URLS, 'r', encoding='utf-8') as f_res:\n",
    "        n_none_urls = 0\n",
    "        for url_rurl in f_res.read().splitlines():\n",
    "            split = url_rurl.split('\\t')\n",
    "            if len(split) != 2:\n",
    "                continue\n",
    "\n",
    "            _, rurl = url_rurl.split('\\t')\n",
    "            if rurl == 'None':\n",
    "                n_none_urls += 1\n",
    "        return n_none_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 37804 urls that could not have been resolved.\n"
     ]
    }
   ],
   "source": [
    "n_none = get_none_urls_count()\n",
    "print(f'There are {n_none} urls that could not have been resolved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods are used to resolve shortened links: they retrieve a batch of shortened links and resolve them. The requests are made in parallel using futures to speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single execution\n",
    "resolve_googl_urls(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple executions\n",
    "from IPython.display import clear_output\n",
    "\n",
    "iter = 0\n",
    "while True:\n",
    "    print(f'Iteration {iter}')\n",
    "    resolve_googl_urls(500)\n",
    "    if iter % 2 == 0:\n",
    "        clear_output(wait=True)\n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we aim to fix previously failed requests by requesting their links again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1397372it [5:11:54, 74.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 7105 urls, 30699 urls could not be retrieved. If the number of errors is too high, consider waiting a few moments before retrying.\n"
     ]
    }
   ],
   "source": [
    "fix_resolved_googl_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting gateways for site 'https://goo.gl'.\n",
      "Deleted 10 endpoints with for site 'https://goo.gl'.\n",
      "Deleting gateways for site 'http://goo.gl'.\n",
      "Deleted 10 endpoints with for site 'http://goo.gl'.\n"
     ]
    }
   ],
   "source": [
    "for url, gateway in url_gateways:\n",
    "    gateway.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this notebook, the Bitly links are resolved and saved in a new file named `googl_resolved_urls.tsv`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('spark')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58222ebbc97bc15a7dd406d2f7b4dd2466104a906450c8e9dcb294b8e2a99cab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
